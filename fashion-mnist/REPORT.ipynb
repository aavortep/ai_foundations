{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e03833c-57f9-4e81-8c17-1da93a670530",
   "metadata": {},
   "source": [
    "# Supervised classification\n",
    "## Problem\n",
    "\n",
    "Write a classifier for the FashionMNIST dataset consisting of 28x28 grayscale images of fashion accessories.\n",
    "\n",
    "Classifiers to use:\n",
    "1. SVM (Support Vector Machine) with linear, polynomial of degree 2, and RBF kernels;\n",
    "2. Random forests with varying number of trees;\n",
    "3. k-NN (k Nearest Neighbors) with varying k;\n",
    "4. Multi-Layer Perceptron (MLP) with varying number and size of hidden layers.\n",
    "\n",
    "For each classifier 10 folds cross validation (CV) should be used.\n",
    "\n",
    "## SVM\n",
    "\n",
    "For SVM implementation scikit-learn library was used. Some of used classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84ee26a-529f-40e7-93d8-dc517915ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  # for polynomial and RBF kernels\n",
    "from sklearn.svm import LinearSVC  # for linear kernel\n",
    "from sklearn.preprocessing import StandardScaler  # scaling the images\n",
    "from sklearn.decomposition import PCA  # for speeding up the training\n",
    "from sklearn.pipeline import Pipeline  # for setting the classifier model\n",
    "from sklearn.model_selection import GridSearchCV  # for cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d3f47-80fd-4745-83c1-a900caf91df4",
   "metadata": {},
   "source": [
    "### Experimental setup\n",
    "\n",
    "Linear kernel has only one parameter C which is how strictly SVM tries to avoid misclassification.\n",
    "\n",
    "* Small C => wider margin, more tolerance for mistakes;\n",
    "* Large C => narrow margin, tries to classify everything correctly.\n",
    "\n",
    "However too large value of C can cause overfitting. So, the best value of C will be chosen after cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f988477-6a98-4706-9426-95edac34866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=100)),\n",
    "    (\"svm\", LinearSVC(dual=False, max_iter=10000))\n",
    "])\n",
    "linear_params = {\"svm__C\": [0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e3675-7b0a-4391-ad29-0529919e8431",
   "metadata": {},
   "source": [
    "Polynomial kernel uses two more parameters (apart from C): $\\gamma$ and coef0.\n",
    "\n",
    "$\\gamma$ controls how strongly two points influence each other:\n",
    "* Large $\\gamma$ => kernel focuses on points very close to each other => highly curved, very wiggly boundaries;\n",
    "* Small $\\gamma$ => smoother, more general boundary.\n",
    "\n",
    "Coef0 controls how much the model uses interaction term (dot product) or constant term (bias):\n",
    "* Small coef0 => model emphasizes the dot product;\n",
    "* Large coef0 => model gives more weight to the constant term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c9ee81f-0212-49be-a924-687550428c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=100)),\n",
    "    (\"svm\", SVC(kernel=\"poly\", degree=2))\n",
    "])\n",
    "poly_params = {\n",
    "    \"svm__C\": [0.01, 0.1, 1, 10],  # less values then in linear case in order to speed up the training a bit\n",
    "    \"svm__coef0\": [0, 0.5, 1],\n",
    "    \"svm__gamma\": [\"scale\", \"auto\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1c2dc-f347-46da-b137-1d6a919c5532",
   "metadata": {},
   "source": [
    "RBF kernel uses only C and $\\gamma$ parameters.\n",
    "\n",
    "$\\gamma$ controls how far the influence of a single training point spreads:\n",
    "* Large $\\gamma$ => influence only on nearby points => very wiggly and complex boundary;\n",
    "* Small $\\gamma$ => influence on a wider area => smooth boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b38de1e-f0b8-4843-934f-4d3f57527c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=100)),\n",
    "    (\"svm\", SVC(kernel=\"rbf\"))\n",
    "])\n",
    "rbf_params = {\n",
    "    \"svm__C\": [0.1, 1, 10],\n",
    "    \"svm__gamma\": [\"scale\", \"auto\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfdce1e-7e2f-44e7-9639-c5fbf46e1c94",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Linear kernel:\n",
    "![image](./img/svm_linear.png)\n",
    "Quadratic polynomial kernel:\n",
    "![image](./img/svm_poly.png)\n",
    "RBF kernel:\n",
    "![image](./img/svm_rbf.png)\n",
    "\n",
    "As we can see, test prediction time was increasing with every type of kernel because of the increasing complexity and amount of computations (linear kernel is the easiest and the most basic one, therefore needs much less time than others). However the time spent on cross validation in RBF kernel was less than in polynomial one most likely due to less amount of varying parameters values. If we add 4 more values in a list of values for C or gamma (making the total number of possible parameters values the same as in polynomial kernel), CV time in RBF would have also been significantly higher than in polynomial case.\n",
    "\n",
    "In the same time the classification performance was getting better with every type of kernel rising from 0.8299 in linear to 0.8922 in RBF which justifies the reason to use these kernels instead of polynomial.\n",
    "\n",
    "From classification report and confusion matrix it is also noticeable that the algorithm had the worst classification performance for the 6th class of images often confusing it with the 0th one and vice versa (which are shirt and T-shirt respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d10ea-4495-48d3-99ee-64687dfd0aae",
   "metadata": {},
   "source": [
    "## Random forests\n",
    "\n",
    "For the model of random forest classifier RandomForestClassifier of scikit-learn was used (apart from some already mentioned classes of the same library).\n",
    "\n",
    "### Experimental setup\n",
    "\n",
    "Random forest classifiers use the number of decision trees as the parameter. Each tree sees a random subset of the training data, considers only a random subset of features (pixels) and gives its own prediction. The forest chooses the label with the most votes. So the more decision trees we have, the stronger final prediction will be (however too large amount of trees may result also in overfitting and increasing computational requirements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15943b7c-3b0f-4282-a88a-7db8d328468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 300]  # numbers of trees\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 10 folds cross validation\n",
    "grid = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c630a1d1-077f-4f71-a559-56ef2fc8a026",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "![image](./img/forest.png)\n",
    "\n",
    "As we can see, for random forests the time needed for cross validation and final training is less than for SVM with linear kernel while having higher classification performance than linear one.\n",
    "\n",
    "This in general is due to the fact that random forest builds many simple decision trees each of which considers only part of the data and features and it is easy to parallelize because trees are built independently. While linear SVM solves one big expensive task and is limited for parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba3190-8f8e-4235-997b-32c8cb873fc5",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors (k-NN)\n",
    "\n",
    "For the model of k-NN classifier KNeighborsClassifier of scikit-learn was used along with KFold to shuffle folds for cross validation (apart from some already mentioned classes of the same library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f402242d-e7a4-4d70-9e5d-873d03883878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee87b0b-119d-436b-a0c4-e5d81f615f1b",
   "metadata": {},
   "source": [
    "### Experimental setup\n",
    "\n",
    "K-NN is a method that looks at some most similar items it's seen before and chooses the majority label. So it uses k to determine how many neighbors to look at:\n",
    "* Small k => sensitive to noise;\n",
    "* Large k => smoother decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8496b146-f847-4f0d-9592-495087269c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_neighbors\": [1, 3, 5, 7, 9, 11]  # different values of k\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# 10 folds cross validation\n",
    "grid = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    cv=KFold(n_splits=10, shuffle=True, random_state=42),  # shuffle folds, so that each fold contains a proper mix of classes\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236dd3e-ed49-4b21-bbc2-84fc81b3114a",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "![image](./img/k-nn.png)\n",
    "\n",
    "As we can see, in this case the time needed for cross validation and final training is significantly less than in all previous methods. While the classification performace is higher than in SVM with linear kernel but less than in others.\n",
    "\n",
    "Such a good time performance is achieved by the fact that k-NN doesn't have any weights or complex equations, it just stores the dataset and compares.\n",
    "\n",
    "As for classification performance, it may be lower than in some other methods because k-NN treats all input features as equally important, even if some of them are useless or misleading. So features that have nothing to do with the true class can still influence the distance calculation and therefore the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97592144-2ce8-483a-aed2-2ff501ed80f4",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "For MLP implementation pytorch library was used. Some of used classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e61fc0d4-d0d3-4e6d-b7d1-4603196d1348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # for different neural network methods\n",
    "import torch.optim as optim  # for adjusting weights\n",
    "from torch.utils.data import DataLoader, Subset  # for manipulations with dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058a2bc-a8fc-45aa-aa5a-342bb0f609f0",
   "metadata": {},
   "source": [
    "### Experimental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bae8964-544d-4d97-80e4-6ba20968ed55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE} device\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LR = 1e-3  # learning rate\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0b8cb-63eb-418d-8d0a-61558c76a4df",
   "metadata": {},
   "source": [
    "Batch is a small chunk of data used for processing. During one epoch all batches are processed. Learning rate determines how much the weights change at each update.\n",
    "\n",
    "As for the device, in my case it's mps which is a GPU for Apple Silicon devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dd87aa9-c675-4df9-8458-81b3450a3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# varying number and size of hidden layers\n",
    "param_grid = [\n",
    "    {\"hidden_layers\": [128]},\n",
    "    {\"hidden_layers\": [256]},\n",
    "    {\"hidden_layers\": [128, 64]},\n",
    "    {\"hidden_layers\": [256, 128]},\n",
    "]\n",
    "\n",
    "# set cross validation with 10 shuffled folds\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4131465-a184-4974-87fc-6a569f9e6390",
   "metadata": {},
   "source": [
    "Each layer in MLP takes simple patterns and combines them into more complex patterns.\n",
    "* 1 hidden layer => simple patterns;\n",
    "* 2 hidden layers => patterns of patterns (more expressive).\n",
    "\n",
    "The size of hidden layers (number of neurons in them) controls how rich the internal representation is:\n",
    "* Few neurons => simpler patterns;\n",
    "* Many neurons => complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99377cb-9fdf-45ca-8be6-779976bc9468",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Cross validation:\n",
    "\n",
    "![image](./img/mlp_cv.png)\n",
    "\n",
    "Final training and testing:\n",
    "\n",
    "![image](./img/mlp_res.png)\n",
    "\n",
    "As we can see, cross validation process in MLP is much slower than in k-NN, random forests and SVM with linear kernel but still faster than SVM with polynomial and RBF kernels. While its test accuracy is better than in those first 3 mentioned methods but worse than the rest of SVM kernels.\n",
    "\n",
    "The time spent in MLP is still less than in some other methods due to linear growth of training time (instead of quadratic or worse in SVM), mini-batch training and GPU acceleration.\n",
    "\n",
    "Test accuracy in case of MLP is worse than in SVM with polynomial and RBF kernels because in general it has many free parameters which makes it easier to underfit or overfit. SVMs are more strongly regularized by design and penalize complex boundaries automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a8901-6280-4cb0-a07a-2e84fa191cea",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Overall, SVM classifier with polynomial and RBF kernels seems to have the best test accuracy among all of these implemented methods. But in the same time it spends really huge amount of time on training and cross validation.\n",
    "\n",
    "In terms of time performance k-NN classifier seems to be the best. However it has worse accuracy than most of the other methods.\n",
    "\n",
    "So, in general, if the time is very important, it's better to use random forests as they spend much less time on training and CV than MLP or SVM and have sufficient accuracy.\n",
    "\n",
    "In the opposite case it's better to use MLP because it has pretty good accuracy in comparison with most of the other methods and spend less time than SVM with polynomial or RBF kernel.\n",
    "\n",
    "SVM with linear kernel is better to use on the dataset which is clearly linearly separable. Otherwise it has too low accuracy in comparison with other methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
